{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEOTQ9ujneim"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN Model Class\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    # Start with 3 convolutional layers\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
        "\n",
        "    # Number of features * size of image at the end which is: 128 / 2^3 (3 pooling layers, each divide by 2)\n",
        "    self.fc1 = nn.Linear(128*16*16, 128)\n",
        "\n",
        "  def forward(self, X):\n",
        "    # Steps: Conv1 -> Pool -> Conv2 -> Pool -> Conv3 -> Pool -> Outputs\n",
        "\n",
        "    # Set out convolutional and pooling layers\n",
        "    X = F.relu(self.conv1(X))\n",
        "    X = F.max_pool2d(X, kernel_size=2, stride=2)\n",
        "\n",
        "    X = F.relu(self.conv2(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "\n",
        "    X = F.relu(self.conv3(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "\n",
        "    # Flatten to a 1D feature vector\n",
        "    X = X.view(-1, 128*16*16)\n",
        "\n",
        "    # Fully connected layer\n",
        "    X = F.relu(self.fc1(X))\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "Ae18uicAr1TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "    # Runs constructor method to initialize RNN class\n",
        "    super(RNN, self).__init__()\n",
        "    # Use size of feature vectors, size of hidden state, number of LSTM layers, while batch_first ensures batch_size is first dimension for input and output tensors\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    # Initializes a fully connected layer, maps hidden state to 1 (binary classification)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    # Initializes sigmoid activation function to convert output to probabilities\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # Define as attribute of the class so can use in forward()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, X):\n",
        "    # Initializes hidden state, creates tensor of zeros with shape (1, batch size, hidden size), then moves tensor to same GPU as X\n",
        "    h0 = torch.zeros(1, X.size(0), self.hidden_size).to(X.device)\n",
        "    # Initializes cell state, same as hidden state\n",
        "    c0 = torch.zeros(1, X.size(0), self.hidden_size).to(X.device)\n",
        "    # Performs forward pass through LSTM using initial hidden and cell states, creates out as output tensor for every frame's step\n",
        "    out, _ = self.lstm(X, (h0, c0))\n",
        "    # Applies fully connected layer to the output tensor, selects just the information from the last frame of the sequence\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    # Applies sigmoid activation to the output, converts to probabilities for binary classification\n",
        "    out = self.sigmoid(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "rHVBeNayMd0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_RNN(nn.Module):\n",
        "  def __init__(self, cnn, rnn):\n",
        "    super(CNN_RNN, self).__init__()\n",
        "    self.cnn = cnn\n",
        "    self.rnn = rnn\n",
        "\n",
        "  def forward(self, X):\n",
        "    batch_size, sequence_length, c, h, w = X.size()\n",
        "    cnn_out = []\n",
        "    for i in range(sequence_length):\n",
        "      cnn_out.append(self.cnn(X[:, i, :, :, :]))\n",
        "    cnn_out = torch.stack(cnn_out, dim=1) # Adds dimension for sequence length to the tensor\n",
        "    rnn_out = self.rnn(cnn_out)\n",
        "    return rnn_out"
      ],
      "metadata": {
        "id": "GmWcFK6MaURD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_frames(path):\n",
        "  # Initialize sequence and label arrays\n",
        "  sequences = []\n",
        "  labels = []\n",
        "\n",
        "  # Initialize dictionary to keep track of pitches labelled\n",
        "  pitch_dict = {}\n",
        "\n",
        "  items = os.listdir(path)\n",
        "  sorted_dir = sorted(items)\n",
        "  item = 0\n",
        "\n",
        "  # Loop through all frames\n",
        "  for frame in sorted_dir:\n",
        "    # Ensure dealing with .png files\n",
        "    if frame.endswith('.png'):\n",
        "      # Get label, pitch number, and frame number\n",
        "      label_str = frame.split('_')[0]\n",
        "      pitch_num = frame.split('_')[-4]\n",
        "      frame_num = int(frame.split('_')[-2])\n",
        "\n",
        "      if label_str != 'Hit':\n",
        "        pitch_num = int(pitch_num)\n",
        "        pitch_num += 200 # Since there are 200 hits, this will make sure the Hits and No Hits are separated\n",
        "        pitch_num = str(pitch_num)\n",
        "\n",
        "      # Normalize pixel values for NN\n",
        "      frame_path = os.path.join(path, frame)\n",
        "      frame = cv2.imread(frame_path)\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "      frame = frame.astype(np.float32) / 255.0\n",
        "\n",
        "      # Add the pitch to the dictionary if its not already there\n",
        "      if pitch_num not in pitch_dict:\n",
        "        # Label the pitch\n",
        "        label = 1 if label_str == 'Hit' else 0\n",
        "\n",
        "        # Add to dictionary\n",
        "        pitch_dict[pitch_num] = {'frame': [], 'label': label}\n",
        "\n",
        "\n",
        "      pitch_dict[pitch_num]['frame'].append(frame)\n",
        "\n",
        "  for pitch_num, pitch_data in pitch_dict.items():\n",
        "    frames = pitch_data['frame']\n",
        "    if len(frames) == 7:\n",
        "      sequences.append(np.array(frames))\n",
        "      labels.append(pitch_data['label'])\n",
        "\n",
        "  return np.array(sequences), np.array(labels)"
      ],
      "metadata": {
        "id": "z041PChB0Uq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "home = Path.home()\n",
        "data_path = os.path.join(home, 'Downloads', 'Wii_Baseball_CNN_RNN', 'Prepared_Frames')\n",
        "\n",
        "sequences, labels = prepare_frames(data_path)\n",
        "\n",
        "train_sequences, val_sequences, train_labels, val_labels = train_test_split(sequences, labels, test_size=0.2)\n",
        "\n",
        "train_sequences = torch.tensor(train_sequences, dtype=torch.float32).unsqueeze(1)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "val_sequences = torch.tensor(val_sequences, dtype=torch.float32).unsqueeze(1)\n",
        "val_labels = torch.tensor(val_labels, dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "giYUIz8ji9e_",
        "outputId": "f81c6d35-2d8a-4ef4-ac83-ea24b2963176",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/Users/jamesgough/Downloads/Wii_Baseball_AI_Project/Prepared_Frames'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8a84fede97cf>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/jamesgough/Downloads/Wii_Baseball_AI_Project/Prepared_Frames'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-20309df4c49c>\u001b[0m in \u001b[0;36mprepare_frames\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Loop through all frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Ensure dealing with .png files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/jamesgough/Downloads/Wii_Baseball_AI_Project/Prepared_Frames'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_sequences, train_labels)\n",
        "val_dataset = TensorDataset(val_sequences, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define a function to check the shape of the data in the DataLoader\n",
        "def check_dataloader_shapes(dataloader, name):\n",
        "    for inputs, labels in dataloader:\n",
        "        print(f'{name} - Inputs shape: {inputs.shape}')\n",
        "        print(f'{name} - Labels shape: {labels.shape}')\n",
        "        break  # Only need to check the shape for one batch\n",
        "\n",
        "# Check the shape of data in train_loader and val_loader\n",
        "check_dataloader_shapes(train_loader, 'Train Loader')\n",
        "check_dataloader_shapes(val_loader, 'Val Loader')"
      ],
      "metadata": {
        "id": "0trIW46b5JGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cnn():\n",
        "    # Create a CNN model\n",
        "    cnn_model = CNN()\n",
        "\n",
        "    # Create a sample input tensor with shape (batch_size, channels, height, width)\n",
        "    sample_input = torch.randn(1, 1, 128, 128)  # Example input\n",
        "\n",
        "    # Forward pass through the CNN\n",
        "    output = cnn_model(sample_input)\n",
        "\n",
        "    # Check the output shape\n",
        "    print(f'CNN Output shape: {output.shape}')  # Should be (batch_size, 128)\n",
        "\n",
        "test_cnn()"
      ],
      "metadata": {
        "id": "L6Lt6QlHW2h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_rnn():\n",
        "    # Create an RNN model\n",
        "    sequence_length = 7\n",
        "    input_size = 128  # Size of feature vectors produced by CNN\n",
        "    hidden_size = 128\n",
        "    output_size = 1  # Binary classification (Hit or No Hit)\n",
        "\n",
        "    rnn_model = RNN(input_size, hidden_size, output_size)\n",
        "\n",
        "    # Create a sample input tensor with shape (batch_size, sequence_length, input_size)\n",
        "    sample_input = torch.randn(32, sequence_length, input_size)  # Example input\n",
        "\n",
        "    # Forward pass through the RNN\n",
        "    output = rnn_model(sample_input)\n",
        "\n",
        "    # Check the output shape\n",
        "    print(f'RNN Output shape: {output.shape}')  # Should be (batch_size, output_size)\n",
        "\n",
        "test_rnn()"
      ],
      "metadata": {
        "id": "tLPXEGHzW-Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cnn_rnn():\n",
        "    # Create CNN and RNN models\n",
        "    cnn_model = CNN()\n",
        "    sequence_length = 7\n",
        "    input_size = 128  # Size of feature vectors produced by CNN\n",
        "    hidden_size = 128\n",
        "    output_size = 1  # Binary classification (Hit or No Hit)\n",
        "\n",
        "    rnn_model = RNN(input_size, hidden_size, output_size)\n",
        "\n",
        "    # Create a combined CNN-RNN model\n",
        "    combined_model = CNN_RNN(cnn_model, rnn_model)\n",
        "\n",
        "    # Create a sample input tensor with shape (batch_size, sequence_length, channels, height, width)\n",
        "    sample_input = torch.randn(32, sequence_length, 1, 128, 128)  # Example input\n",
        "\n",
        "    # Forward pass through the combined CNN-RNN model\n",
        "    output = combined_model(sample_input)\n",
        "\n",
        "    # Check the output shape\n",
        "    print(f'Combined CNN-RNN Output shape: {output.shape}')  # Should be (batch_size, output_size)\n",
        "\n",
        "test_cnn_rnn()"
      ],
      "metadata": {
        "id": "1szQ_7TtXDSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = CNN()\n",
        "\n",
        "sequence_length = 7   # 7 pitches\n",
        "input_size = 128      # Size of feature vectors produced by CNN\n",
        "hidden_size = 128     # Number of features in the hidden state of RNN\n",
        "output_size = 1       # Binary classification (Hit or No Hit)\n",
        "\n",
        "rnn_model = RNN(sequence_length, input_size, hidden_size, output_size)\n",
        "\n",
        "combined_model = CNN_RNN(cnn_model, rnn_model)"
      ],
      "metadata": {
        "id": "XO5XREpxAK0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda_is_available() else 'cpu')\n",
        "combined_model.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(combined_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "nMCMfeAEDcFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  combined_model.train()\n",
        "  train_loss = 0.0\n",
        "\n",
        "  for inputs, labels in tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n",
        "    inputs, labels = inputs.to(device), labels.to(device).float()   # Move data to device\n",
        "\n",
        "    optimizer.zero_grad()   # Clear all gradients\n",
        "    outputs = combined_model(inputs)    # Forward pass\n",
        "    loss = criterion(outputs.squeeze(), labels)   # Computes loss for every sample in batch, then calculates averages losses to give one scalar value\n",
        "    loss.backward()   # Calculates gradients\n",
        "    optimizer.step()    # Updates model parameters\n",
        "\n",
        "    train_loss += loss.item() * inputs.size(0)  # Multiplies average loss of batch by sequences in batch to get total loss over the whole batch\n",
        "\n",
        "  train_loss = train_loss / len(train_dataset)   # Calculates average loss for every sample in this epoch\n",
        "\n",
        "  combined_model.eval()\n",
        "  val_loss = 0.0\n",
        "  val_corr = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in tqdm(val_loader, f'Validation Epoch {epoch+1}/{num_epochs}'):\n",
        "      inputs, labels = inputs.to(device), labels.to(device).float() # Move data to device and convert to float\n",
        "      outputs = combined_model(inputs)\n",
        "      loss = criterion(outputs.squeeze(), labels)   # outputs.squeeze() removes dimensions of size 1, leaves batch_size\n",
        "      val_loss += loss.item() * inputs.size(0)\n",
        "      predictions = outputs.squeeze().round()   # Make prediction\n",
        "      val_corr += (predictions == labels).sum().item()    # Adds 1 if prediction matches the label, use .item() to extract scalar from the tensor\n",
        "\n",
        "  val_loss = val_loss / len(val_dataset)\n",
        "  val_acc = val_corr / len(val_dataset)\n",
        "  val_acc_percent = val_acc * 100\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{num_epochs},'\n",
        "        f'Train Loss: {train_loss:.4f},'\n",
        "        f'Val Loss: {val_loss:.4f},'\n",
        "        f'Val Accuracy: {val_acc_percent:.4f}%')"
      ],
      "metadata": {
        "id": "bKT0UnAOEF22"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}